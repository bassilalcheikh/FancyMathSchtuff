\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\newtheorem{thm}{Theorem}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{amsfonts}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]

\title{Bayes' Theorem and the Basis for Statistical Inference}
\author{Bassil Alcheikh}
% Notes taken from: https://web.stanford.edu/~dntse/classes/cs70_fall09/n11.pdf
\begin{document}
\maketitle

\section{Introduction}
\textit{Statistical inference} is the process of analyzing data to deduce properties of an underlying distribution. As more information becomes available, \textit{Bayesian inference} specifically aims to update the probability for a given hypothesis. Bayes' Rule makes this sort of inference possible. 
\section{Definitions}

\begin{defn} % Definition 2.1: Conditional Probability
The \textbf{conditional probability} of an event $A$ given an event $B$ is equal to:
$$P(A|B) = \frac{P(A \cap B)}{P(B)},$$
where $P(B) \neq 0$. This can be shown intuitively with a Venn diagram.
\end{defn}
\bigbreak
\begin{defn} % Definition 2.2: Total Probability 
\textbf{Total Probability.} For any two events $A, B$:
\begin{align*} 
\\P(A) &= P(A \cap B)+P(A\cap\overline{B})
\\{} &= P(A|B)\cdot P(B) + P(A|\overline{B})\cdot P(\overline{B})
\\{} &= P(A|B)\cdot P(B) + P(A|\overline{B})\cdot (1-P(B))
\end{align*} 
\end{defn}
\bigbreak
\begin{defn} % Definition 2.3: Independence
\textbf{Independence.} Two events $A, B$ are \textit{independent} if: 
$$P(A\cap B) = P(A) \cdot P(B).$$
Note what this implies: 
$$P(A\cap B) = P(A) \cdot P(B) \implies P(A) = P(A|B), \; P(B) = P(B|A)$$
\end{defn}
\bigbreak
\begin{defn} % Definition 2.4: Disjoint Events
\textbf{Disjoint Events/"mutually exclusive."} Finite events $A_i$ for $i: 1 \leq i \leq n$ are \textit{disjoint} if no pair contain a common sample point. In layman's terms, no two events can occur at the same time.
\end{defn}

% THEOREM 1: Bayes' Rule
\begin{thm} \textbf{Bayes' Rule.} For any two events $A$ and $B$, 
$$P(A|B) = \frac{P(B|A)\cdot P(A)}{P(B)}, \quad \text{where } P(B) \neq 0.$$ 
\end{thm}
% Proof: 
\begin{proof} Consider events $A$ and $B$, where $P(A)$, $P(B) > 0$. According to the definition of conditional probability, 
\begin{align*} 
\\P(A|B) &= \frac{P(A \cap B)}{P(B)} \implies P(A\cap B) = P(A|B)\cdot P(B)
\\{}&\implies P(A\cap B) = P(B|A) \cdot P(A)
\\{}&\implies P(A|B)\cdot P(B) = P(B|A) \cdot P(A)
\\ \therefore \quad P(A|B) &= \frac{P(B|A)\cdot P(A)}{P(B)} 
\end{align*}
\end{proof}
% THEOREM 2: Inclusion/Exclusion
\begin{thm} \textbf{Inclusion/Exclusion Principle.} For some finite set of events $A_i$ where $1 \leq i \leq n$ in some probability space, the probability of their union is equal to: 
$$P\bigg(\bigcup_{i=1}^n A_i \bigg) = \sum_{i=1}^n A_i - \sum_{\{i,j\}} [A_i \cap A_j] + \sum_{\{i,j,k\}} [A_i \cap A_j \cap A_k] - \cdots \pm P\bigg(\bigcap_{i=1}^n A_i \bigg),$$
where $\{i, j\}$ denotes all unordered pairs with $i \neq j$, $\{i, j, k\}$ denotes all unordered triples of distinct elements, and so forth.
\end{thm}
% Proof: 
\begin{proof} INSERT PROOF HERE
\begin{align*} 
\\ {} &= {}
\\ {} &= {}
\end{align*}
\end{proof}
\section{Examples}
\begin{exmp}
\textit{Finding mean and variance for a binomial distribution.}
\bigbreak Consider binomial random variable $X$:
$$m(t)=[(1-p)+pe^t]^n$$
$$m'(t)=n\cdot[(1-p)+pe^t]^{n-1}\cdot pe^t$$
\quad \: Evaluated at $t=0$, we find the mean:
$$m'(0)=\mu = np.$$
\quad \: To find the variance, we aim to compute 
$$V(x) = E(x^2)-[E(x)]^2$$
\quad \: This means we will have to take the second derivative of $m(t)$ to get $E(x^2)$:
$$m''(t)=n\cdot[(1-p)+pe^t]^{n-1}\cdot (pe^t)+(pe^t)^2\cdot n(n-1)\cdot[(1-p)+pe^t]^{n-2}$$
\quad \: Evaluated at $t=0$, we find $E(x^2)$ and subtract $[E(x)]^2$: 
$$m''(0) - (m'(0))^2 = np+p^2\cdot n(n-1)-(np)^2=np+(np)^2-p^2n-(np)^2$$
$$\therefore \quad V(x) = np(1-p)$$
\end{exmp}
\end{document}
